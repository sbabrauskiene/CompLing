{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AnnSenina/Python_for_CL/blob/main/notebooks/Python_9_%D0%BF%D1%80%D0%B5%D0%BF%D1%80%D0%BE%D1%86%D0%B5%D1%81%D1%81%D0%B8%D0%BD%D0%B3_NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKMFDWff6OJ9"
   },
   "source": [
    "#Препроцессинг NLTK\n",
    "\n",
    "Подготовка текста для анализа\n",
    "\n",
    "<s>Ведь мы c вами все знаем, что на самом деле цифровой анализ текста - это и есть частотности слов</s> :)\n",
    "\n",
    "Используются материалы из тетрадок Д.Скоринкина, А. Хорошевой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "MQfRhCxL3yA1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "давайте нормализуем этот текст\n"
     ]
    }
   ],
   "source": [
    "# кое-что мы уже умеем, например: \n",
    "\n",
    "print(\" Давайте нормализуем этот текст!      \".lower().strip(\" )?!.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "HL8oD4Rm6uR5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mr', 'and', 'mrs', 'dursley', 'of', 'number', 'four', 'privet', 'drive', 'were', 'proud', 'to', 'say', 'that', 'they', 'were', 'perfectly', 'normal', 'thank', 'you', 'very', 'much', 'they', 'were', 'the', 'last', 'people', 'you’d', 'expect', 'to', 'be', 'involved', 'in', 'anything', 'strange', 'or', 'mysterious', 'because', 'they', 'just', 'didn’t', 'hold', 'with', 'such', 'nonsense']\n",
      "['привет', 'здравствуй', 'мир']\n"
     ]
    }
   ],
   "source": [
    "# функция, убирающая пунктуацию\n",
    "\n",
    "import string\n",
    "\n",
    "def normalize(text):\n",
    "    normalized = text.lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
    "    return normalized\n",
    "\n",
    "text = \"Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you’d expect to be involved in anything strange or mysterious, because they just didn’t hold with such nonsense.\"\n",
    "\n",
    "print(normalize(text))\n",
    "print(normalize('Привет, здравствуй мир!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZ3Pm9ey8Qgr"
   },
   "source": [
    "Метод .maketrans() имеет три аргумента и создает таблицу перевода:\n",
    "\n",
    "* какие символы переводить (первый аргумент)\n",
    "* в какие переводить (второй аргумент)\n",
    "* какие символы удалять (третий аргумент)\n",
    "Метод .translate() использует таблицу перевода, чтобы превратить символы в новые символы.\n",
    "\n",
    "В нашем случае, в методе .maketrans() первые два аргумента мы оставляем пустыми (ничто переводится в ничто), а аргумент для удаления определяем как строку punctuation, в которой содержатся все символы пунктуации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPvYUki89ZHp"
   },
   "source": [
    "#Задача 1: вспомним то, что уже умеем"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Yf1XqpY-Cdw"
   },
   "source": [
    "Определяем лучшего писателя отзывов. \n",
    "У вас есть четрые отзыва на фильм С.Кубрика \"Сияние\". Вам нужно определить, кто из авторов отзывов написал отзыв с наибольшим количеством уникальных слов.\n",
    "    \n",
    "    Для определения вам нужно: (для каждого автора)\n",
    "\n",
    "    1. Предобработать строку, сведя все к нижнему регистру, убрать пунктуацию.\n",
    "    2. Превратить строку в список \n",
    "    3. Оставить уникальные элементы в списке (превратить список во множество)\n",
    "    4. Определить размер такого множества\n",
    "    \n",
    "У кого из авторов количество уникальных слов наибольшее?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "sM8a9Tlw-AEu"
   },
   "outputs": [],
   "source": [
    "paul = \"\"\"When this film first came out in 1980, I remember going to see it on opening night. This movie just scared the life out of me, which is what still happens every time\n",
    "I rent the video for a re-watch.I have seen The Shining at least six or seven times, and I still believe it to be simultaneously and paradoxically one of the most frightening and yet funniest films I've ever seen. Frightening because of the extraordinarily effective use of long shots to create feelings of isolation, convex lens shots to enhance surrealism, and meticulously scored music to bring tension levels to virtually unbearable levels. And \\\"funny\\\" because of Jack Nicholson's outrageous and in many cases ad-libbed onscreen antics. It never ceases to amaze me how The Shining is actually two films in one, both a comedy AND a horror flick. Ghostly apparitions of a strikingly menacing nature haunt much of the first half of the film, which gradually evolve into ever more serious physical threats as time progresses. Be that as it may, there is surprisingly little violence given the apparent intensity, but that is little comfort \n",
    "for the feint of heart as much of the terror is more implied than manifest. The Shining is a truly frightening movie that works symbolically on many levels, but is basically  about human shortcomings and the way they can be exploited by unconscious forces combined with weakness of will. This film scares the most just by using suggestion to turn your own imagination against you. The Shining is a brilliant cinematic masterpiece, the likes of which have never been seen before or since. Highly, highly recommended.\"\"\"\n",
    "\n",
    "jane = \"\"\"Chilling, majestic piece of cinematic fright, this film combines all the great elements of an intellectual thriller, with the grand vision of a director who has the instinctual capacity to pace a moody horror flick within the realm of his filmmaking genius that includes an eye for the original shot, an ice-cold soundtrack and an overall sense of dehumanization. This movie cuts through all the typical horror\n",
    "movies like a red-poker through a human eye, as it allows the viewer to not only feel the violence and psychosis of its protagonist, but appreciate the seed from which the derangement stems. One of the scariest things for people\n",
    "to face is the unknown and this film presents its plotting with just that thought in mind. The setting is perfect, in a desolate winter hideaway. The quietness of the moment is a character in itself, as the fermenting aggressor in Jack Torrance's mind wallows in this idle time, and breeds the devil's new playground. I always felt like the presence of evil was dormant in all of our minds, with only the circumstances of the moment, and the reasons given therein, needed to wake its violent ass and pounce over its unsuspecting victims. This film is a perfect example of this very thought.\"\"\"\n",
    "\n",
    "kate = \"\"\"What can I say about the scariest movie I have ever seen that has not already been said by others more articulate than yours truly? Do not view this film expecting to see a screen version of the Stephen King novel.\n",
    "Rather, this is a Stanley Kubrick film, and to fully appreciate it one should judge it within the context of Kubrick's entire body of work as a serious filmmaker. Thematically, THE SHINING relates most closely to 2001:\n",
    "A SPACE ODYSSEY, though flourishes of PATHS OF GLORY, A CLOCKWORK ORANGE and BARRY LYNDON do manage to figure prominently in the film's overall technique. In a nutshell (no pun intended), Jack Nicholson and Shelly Duvall co-star with Oregon's Timberline Lodge - enlisted to portray the exterior of the Overlook Hotel - in a story that appears on the surface to be about ghosts and insanity, but deals with issues of child abuse, \n",
    "immortality and duality. What the film might lack initially in terms of coherence is more than made up for in technique. Garrett Brown (the male voice in those old Molson Golden commercials), inventor of the Steadicam,chases young Danny Lloyd through hotel corridors and an amazing snow maze, providing magic-carpet-ride fluidity to scenes that ten years earlier would have been impossible to accomplish. If the film starts off too slow, remember who the director is. This man likes to take his time, and the results are well worth it: incredible aerial shots of the Overlook Hotel; horrific Diane Arbus-inspired twins staring directly at us; portentous room 237 and its treasure trove of terrible secrets; elevators that gush rivers of blood in slow-motion; Jack Torrance's immortality found via the hotel (akin to David Bowman's journey through the Space Gate); and some of the best use of pre-existing music ever assembled for a motion picture.\"\"\"\n",
    "\n",
    "nick = \"\"\"I was never a big fan of horror movies. They usually try cheap tricks to scare their audiences like loud noises and creepy children. They usually lack originality and contain overacting galore. The only horror movie i\n",
    "like was Stir of Echoes with Kevin Bacon. It was well-acted, and had a great story. But it has been joined and maybe even surpassed by Stanley Kubrick's The Shining, quite possibly the scariest movie ever. The movie follows a writer (Jack Nicholson) and his family who agree to watch over a hotel while it is closed for the winter. There were rumors of the place being haunted and the last resident went crazy and murdered his family. But Jack is convinced it will be OK and he can use the quiet to overcome his writer's block. After months of solitude and silence however, Jack becomes a grumpy and later violent. Is it cabin fever or is there something in the hotel that is driving him mad? One of the creepiest parts about the movie is the feeling of isolation that Kubrick makes. The hotel is very silent, and the rooms are huge, yet always empty. It is also eerily calm when Jack's son is riding his bike through the barren hallways. Jack Nicholson's performance is also one of his very best, scaring the hell out of me and making me sure to get out once in awhile. My favorite scene is when he is talking to a ghost from inside a walk-in refrigerator. The Shining is tops for horror movies in my opinion, beating the snot out of crap like the Ring and The Blair Witch Project. It may be a oldie, but is definitely a goodie.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12FjlgZz--bf"
   },
   "outputs": [],
   "source": [
    "# мы можем использовать тройные кавычки (то есть три одинарные кавычки или три двойные кавычки) для строк с одинарными и двойными кавычками\n",
    "# так мы исключаем необходимость экранирования любых кавычек\n",
    "# а еще можем записывать перенос строки без экранирования \\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UnGp8fx8--y7",
    "outputId": "6a3162e2-d95c-44fe-f6ee-f9c862d17663"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long lowset dogs with sturdy bone short legs and a deep chest cardigans are powerful workers of deceptive speed and grace\n"
     ]
    }
   ],
   "source": [
    "# Подсказка: как быстро убрать всю пунктуацию из текста и сразу переводить в нижний регистр\n",
    "\n",
    "import string\n",
    "\n",
    "test = \"Long, low-set dogs with sturdy bone, short legs, and a deep chest, Cardigans are powerful workers of deceptive speed and grace.\"\n",
    "\n",
    "test_clean = test.translate(str.maketrans(\"\", \"\", string.punctuation)).lower()\n",
    "print(test_clean)\n",
    "\n",
    "# подробнее о методе .maketrans()\n",
    "# https://www.w3schools.com/python/ref_string_maketrans.asp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "PVPLIyML_3fh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Самый богатый лексикон у Kate - 208 уникальныых слов.\n"
     ]
    }
   ],
   "source": [
    "# ваше решение ниже:\n",
    "\n",
    "import string\n",
    "\n",
    "def normalize(name):\n",
    "    normalized = name.lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
    "    return normalized\n",
    "\n",
    "names_dict = {'Paul': len(set(normalize(paul))), 'Jane': len(set(normalize(jane))), 'Kate': len(set(normalize(kate))), 'Nick': len(set(normalize(nick)))}\n",
    "\n",
    "#print(names_dict)\n",
    "\n",
    "maxcount = None\n",
    "maxname = None\n",
    "for name,count in names_dict.items():\n",
    "    if maxcount is None or count > maxcount:\n",
    "        maxname = name\n",
    "        maxcount = count\n",
    "        \n",
    "print('Самый богатый лексикон у', maxname, '-', maxcount, 'уникальных слов.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75EnFxyh9CNo"
   },
   "source": [
    "## Давайте попробуем подготовить текст романа \"Преступление и наказание\" к анализу\n",
    "\n",
    "Проведем предобработку текста. Посмотрим на практике, на каком этапе нужна лемматизация."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "Rt48CpY_7G3R"
   },
   "outputs": [],
   "source": [
    "# откроем файл в питоне\n",
    "with open('/Users/Svetlana/Documents/Обучение/CompLinguistics/Python/CompLing/Classes/Dostoevsky_PrestuplenieINakazanie.txt', 'r') as open_file: \n",
    "    text = open_file.read() # считаем файл в строку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "tzuGUmxNB9w2"
   },
   "outputs": [],
   "source": [
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bViid1pcCC2f"
   },
   "source": [
    "Убедимся, что в тексте лежит то что мы ожидаем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "1HDYpUUICBDy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "преступление и наказание \n",
      "\n",
      "роман в шести частях с эпилогом\n",
      "\n",
      "  \n",
      "часть первая\n",
      "\n",
      "i\n",
      "\n",
      "   в начале июля, в \n"
     ]
    }
   ],
   "source": [
    "# начало романа:\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ZcHCSxxGCH1W"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1094386\n"
     ]
    }
   ],
   "source": [
    "# длина всего романа в символах\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSSKHEv_Cu87"
   },
   "source": [
    "Получим слова\n",
    "\n",
    "* Их частотности\n",
    "* Их сочетаемость друг с другом\n",
    "* Их распределение по тексту и т.д."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3zx-YjOEpGV"
   },
   "source": [
    "Мы знаем простой способ токенизировать строку — метод .split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "e11H8M9xC2bK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Давайте', 'еще', 'раз', 'протестируем', 'токенизацию']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_string = 'Давайте еще раз   протестируем            токенизацию'\n",
    "some_string.split() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gts2Q-QcEyk6"
   },
   "source": [
    "Теперь разделим \"Преступление и наказание\" и посчитаем в нем слова — хотя бы примерно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "F_do8eUlEula"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Примерное количество слов в \"Преступлении и наказании\": 176283\n"
     ]
    }
   ],
   "source": [
    "text_list = text.split()\n",
    "print('Примерное количество слов в \"Преступлении и наказании\":', len(text_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "5TRW36KRFBjY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['преступление', 'и', 'наказание', 'роман', 'в', 'шести', 'частях', 'с', 'эпилогом', 'часть', 'первая', 'i', 'в', 'начале', 'июля,', 'в', 'чрезвычайно', 'жаркое', 'время,', 'под', 'вечер,', 'один', 'молодой', 'человек', 'вышел', 'из', 'своей', 'каморки,', 'которую', 'нанимал']\n"
     ]
    }
   ],
   "source": [
    "# посмотрим на верхушку этого списка\n",
    "print(text_list[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "TcUbSQgqFeMZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Аня', 'пришла', 'в', 'Вышку,', 'Никита', 'пришел', 'в', 'Вышку']\n",
      "Вышку,\n",
      "Вышку\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = 'Аня пришла в Вышку, Никита пришел в Вышку'.split()\n",
    "print(tokens)\n",
    "print(tokens[3])\n",
    "print(tokens[7])\n",
    "tokens[3] == tokens[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "jSV0ykmGFkud"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "топором\n",
      "топор,\n",
      "топором...\n",
      "топора.\n",
      "топор\n",
      "топора,\n",
      "топорной\n",
      "топор.\n",
      "топором,\n",
      "топоре\n",
      "топор,\n",
      "топор,\n",
      "топора,\n",
      "топором\n",
      "топора!\n",
      "топор\n",
      "топор)\n",
      "топор,\n",
      "топор.\n",
      "топор\n",
      "топор...\n",
      "топор\n",
      "топор,\n",
      "топор\n",
      "топор\n",
      "топором,\n",
      "топор,\n",
      "топором\n",
      "топор,\n",
      "топор\n",
      "топором;\n",
      "топор\n",
      "топор.\n",
      "топор\n",
      "топор,\n",
      "топор\n",
      "топор\n",
      "топор\n",
      "топор.\n",
      "топор?\n",
      "топоре.\n",
      "топора\n",
      "топор.\n",
      "топор\n",
      "топором\".\n",
      "топором\n",
      "топорами,\n",
      "топором,\n",
      "топором\n",
      "топор\n",
      "топор\n",
      "топора\n",
      "топором\n",
      "топором.\n",
      "топором.\n",
      "топор,\n",
      "топором,\n",
      "топором,\n",
      "топором,\n",
      "топором\n"
     ]
    }
   ],
   "source": [
    "# посмотрим на все слова, в которых есть подстрока 'топор'\n",
    "for token in text_list:\n",
    "  if 'топор' in token:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JSihridMFwQi"
   },
   "source": [
    "### Более умный способ токенизации: делим текст регулярным выражением \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "ygQFU5L3F0N-"
   },
   "outputs": [],
   "source": [
    "import re # разбиваем текст по всем знакам препинания\n",
    "text_list_with_re = re.split(r\"[-@\\s.,)(\\\":;!?–\\n]+\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "W5KBGxzkF_RZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['преступление',\n",
       " 'и',\n",
       " 'наказание',\n",
       " 'роман',\n",
       " 'в',\n",
       " 'шести',\n",
       " 'частях',\n",
       " 'с',\n",
       " 'эпилогом',\n",
       " 'часть',\n",
       " 'первая',\n",
       " 'i',\n",
       " 'в',\n",
       " 'начале',\n",
       " 'июля',\n",
       " 'в',\n",
       " 'чрезвычайно',\n",
       " 'жаркое',\n",
       " 'время',\n",
       " 'под',\n",
       " 'вечер',\n",
       " 'один',\n",
       " 'молодой',\n",
       " 'человек',\n",
       " 'вышел',\n",
       " 'из',\n",
       " 'своей',\n",
       " 'каморки',\n",
       " 'которую',\n",
       " 'нанимал']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list_with_re[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "SczKlOqaGEa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "топором\n",
      "топор\n",
      "топором\n",
      "топора\n",
      "топор\n",
      "топора\n",
      "топорной\n",
      "топор\n",
      "топором\n",
      "топоре\n",
      "топор\n",
      "топор\n",
      "топора\n",
      "топором\n",
      "топора\n",
      "топор\n",
      "топор\n",
      "топор\n",
      "топор\n",
      "топор\n",
      "топор\n",
      "топор\n",
      "топор\n",
      "топор\n",
      "топор\n",
      "топором\n",
      "топор\n",
      "топором\n",
      "топор\n",
      "топор\n",
      "топором\n",
      "топор\n",
      "топор\n",
      "топор\n",
      "топор\n",
      "топор\n",
      "топор\n",
      "топор\n",
      "топор\n",
      "топор\n",
      "топоре\n",
      "топора\n",
      "топор\n",
      "топор\n",
      "топором\n",
      "топором\n",
      "топорами\n",
      "топором\n",
      "топором\n",
      "топор\n",
      "топор\n",
      "топора\n",
      "топором\n",
      "топором\n",
      "топором\n",
      "топор\n",
      "топором\n",
      "топором\n",
      "топором\n",
      "топором\n"
     ]
    }
   ],
   "source": [
    "# проверим, что проблема приклеившейся пунктуации ушла\n",
    "for word in text_list_with_re:\n",
    "  if 'топор' in word:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HH5xf4oiGOHY"
   },
   "source": [
    "### Еще более умный способ: сегментируем текст готовым токенизатором — возьмем его из прекрасной библиотеки для обработки языка NLTK \n",
    "\n",
    "[Документация по NLTK](https://www.nltk.org/) и [книжка](https://www.nltk.org/book/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gE1JTGwiGo9x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk) (4.64.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "# в Colab уже есть, в других средах - запустите ячейку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RFPNuhlAGhPN"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "FYxGahd5UBcx"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Svetlana/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# это подгрузка вспомогательных данных, которые нужны nltk для токенизации\n",
    "from nltk import download\n",
    "download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Zjy_KO28T1Qz"
   },
   "outputs": [],
   "source": [
    "text_2 = \"\"\"Задача NLI важна для компьютерных лингвистов, ибо она позволяет детально рассмотреть, какие языковые явления данная модель понимает хорошо, а на каких – \"плывёт\"; по этому принципу устроены диагностические датасеты SuperGLUE и RussianSuperGLUE. Кроме этого, модели NLI обладают прикладной ценностью по нескольким причинам.\n",
    "Во-первых, NLI можно использовать для контроля качества генеративных моделей. Есть масса задач, где на основе текста X нужно сгенерировать близкий к нему по смыслу текст Y: суммаризация, упрощение текстов, перефразирование, перенос стиля на текстах, текстовые вопросно-ответные системы, и даже машинный перевод. Современные seq2seq нейросети типа T5 (которая в этом году появилась и для русского языка) в целом неплохо справляются с такими задачами, но время от времени лажают, упуская какую-то важную информацию из Х, или, наоборот, дописывая в текст Y что-то нафантазированное \"от себя\". С помощью модели NLI можно проверять, что из X следует Y (то есть в новом тексте нету \"отсебятины\", придуманной моделью), и что из Y следует X (т.е. вся информация, присутствовавшая в исходном тексте, в новом также отражена).\n",
    "Во-вторых, с помощью моделей NLI можно находить нетривиальные парафразы и в целом определять смысловую близость текстов. Для русского языка уже существует ряд моделей и датасетов по перефразированию, но кажется, что можно сделать ещё больше и лучше. В статье Improving Paraphrase Detection with the Adversarial Paraphrasing Task предложили считать парафразами такую пару предложений, в которой каждое логически следует из другого – и это весьма логично. Поэтому модели NLI можно использовать и для сбора обучающего корпуса парафраз (и не-парафраз, если стоит задача их детекции), и для фильтрации моделей, генерирующих парафразы.\"\"\"\n",
    "# текст отсюда - https://habr.com/ru/post/582620/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "NPgNbDEgT177"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Задача', 'NLI', 'важна', 'для', 'компьютерных', 'лингвистов', ',', 'ибо', 'она', 'позволяет']\n"
     ]
    }
   ],
   "source": [
    "print(wordpunct_tokenize(text_2)[:10])\n",
    "# wordpunct_tokenizer разбирает по регулярке - '\\w+|[^\\w\\s]+' \n",
    "# word_tokenize - тоже основан на регулярках, но более умных (учитывается последовательность некоторых символов, символы начала, конца слова и т.д).\n",
    "# для русского языка работает немного хуже, чем для английского"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "7M8E1sxJGwnh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['преступление', 'и', 'наказание', 'роман', 'в', 'шести', 'частях', 'с', 'эпилогом', 'часть', 'первая', 'i', 'в', 'начале', 'июля', ',', 'в', 'чрезвычайно', 'жаркое', 'время', ',', 'под', 'вечер', ',', 'один', 'молодой', 'человек', 'вышел', 'из', 'своей']\n"
     ]
    }
   ],
   "source": [
    "# вернемся к Достоевскому, используем более умную лемматизацию\n",
    "text_list_nltk = word_tokenize(text)\n",
    "print(text_list_nltk[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4nIAbE9HRao"
   },
   "source": [
    "## Посчитаем частотность слов в нашем списке слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vMjWIrgHcFk"
   },
   "source": [
    "Мы могли бы написать цикл, который считает упоминания каждого слова, перебирая в списке токенов слово за словом. А затем посчитать их при помощи словаря. \n",
    "\n",
    "Но! Мы воспользуемся встроенным в Python объектом Counter; он все это сделает сам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPmJIpx9HnhQ"
   },
   "source": [
    "### Counter -- встроенный счетчик частотностей в Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "pY3X7cnOHXq9"
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "4pYvF8oMHqx5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'a': 5, 'b': 4, 'c': 3, 'd': 2, 'e': 1, 'f': 1, 'g': 1, 'h': 1, 'i': 1, 'k': 1})\n",
      "[('a', 5), ('b', 4), ('c', 3)]\n"
     ]
    }
   ],
   "source": [
    "# Counter - это специальный объект, который умеет легко считать повторяющиеся элементы в iterable\n",
    "# Например в строке:\n",
    "char_freqs = Counter('aaaaabbbbcccddefghik') # получим частотности всех элементов строки\n",
    "print(char_freqs)\n",
    "print(char_freqs.most_common(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "sUnjcX-cIAX-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 25337), ('и', 8452), ('.', 8197), ('--', 6073), ('не', 3779), ('в', 3727), ('!', 3265), ('что', 3215), ('он', 2852), ('на', 2407)]\n"
     ]
    }
   ],
   "source": [
    "word_freqs = Counter(text_list_nltk) # отправим в Counter\n",
    "print(word_freqs.most_common(10)) # получим топ 10 слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7oNBTGlINjJ"
   },
   "source": [
    "##Снова удалим пунктуацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "lvELGmdhIfFz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# зачем новый способ?\n",
    "\n",
    "print('john'.isalpha())\n",
    "print('1989'.isalpha())\n",
    "print(','.isalpha())\n",
    "print('диван-кровать'.isalpha()) # из-за дефиса тоже будет false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c73a3bPASykt",
    "outputId": "19bb78d4-f25b-43d0-9efb-1d86f088c5a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# методом maketrans, посмотрим на string.punctuation\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "ZkmeiGNAI1kL"
   },
   "outputs": [],
   "source": [
    "text_without_punkt = [] \n",
    "for word in text_list_nltk:\n",
    "    if word[0].isalpha():\n",
    "        text_without_punkt.append(word)\n",
    "\n",
    "# на практике вы часто встретите строковые включения:\n",
    "# text_clean = [word for word in text_without_punkt if word not in stop_words]\n",
    "# краткая форма записи цикла for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "yQuKu10OJGz3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['преступление', 'и', 'наказание', 'роман', 'в', 'шести', 'частях', 'с', 'эпилогом', 'часть', 'первая', 'i', 'в', 'начале', 'июля', 'в', 'чрезвычайно', 'жаркое', 'время', 'под', 'вечер', 'один', 'молодой', 'человек', 'вышел', 'из', 'своей', 'каморки', 'которую', 'нанимал']\n"
     ]
    }
   ],
   "source": [
    "print(text_without_punkt[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FrbsmxrIJNsx"
   },
   "source": [
    "###Посмотрим на частотность слов без пунктуации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "0xhQQ7QwJRxF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('и', 8452), ('не', 3779), ('в', 3727), ('что', 3215), ('он', 2852), ('на', 2407), ('я', 2395), ('с', 2007), ('а', 1777), ('как', 1622), ('это', 1386), ('его', 1189), ('так', 1172), ('но', 1155), ('же', 1130), ('да', 1055), ('вы', 969), ('всё', 953), ('к', 919), ('она', 901), ('бы', 865), ('было', 786), ('еще', 701), ('у', 697), ('то', 688), ('даже', 680), ('по', 667), ('за', 650), ('ее', 625), ('только', 619)]\n"
     ]
    }
   ],
   "source": [
    "print(Counter(text_without_punkt).most_common(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0jIlYlbJhMc"
   },
   "source": [
    "##Чистим от стоп-слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_Anp2_nJkVb"
   },
   "source": [
    "Можно загрузить свои стоп-слова и удалить их, но проще взять из NLTK: там есть наборы стоп-слов для разных языков\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "c6MXsMVgJg2R"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Svetlana/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "5GGXQfOOJtt9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('russian') \n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "M9EtTRs8J5Jl"
   },
   "outputs": [],
   "source": [
    "text_clean = []\n",
    "for word in text_without_punkt:\n",
    "  if word not in stop_words:\n",
    "    text_clean.append(word)\n",
    "\n",
    "# text_clean = [word for word in text_without_punkt if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "eo7Ee7SEKSj0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Задача', 'NLI', 'важна', 'компьютерных', 'лингвистов', 'ибо', 'позволяет', 'детально', 'рассмотреть', 'какие', 'языковые', 'явления', 'данная', 'модель', 'понимает', 'каких', 'плывёт', 'этому', 'принципу', 'устроены', 'диагностические', 'датасеты', 'SuperGLUE', 'RussianSuperGLUE', 'Кроме', 'модели', 'NLI', 'обладают', 'прикладной', 'ценностью', 'нескольким', 'причинам', 'Во-первых', 'NLI', 'использовать', 'контроля', 'качества', 'генеративных', 'моделей', 'Есть', 'масса', 'задач', 'основе', 'текста', 'X', 'нужно', 'сгенерировать', 'близкий', 'нему', 'смыслу', 'текст', 'Y', 'суммаризация', 'упрощение', 'текстов', 'перефразирование', 'перенос', 'стиля', 'текстах', 'текстовые', 'вопросно-ответные', 'системы', 'машинный', 'перевод', 'Современные', 'seq2seq', 'нейросети', 'типа', 'T5', 'которая', 'году', 'появилась', 'русского', 'языка', 'целом', 'неплохо', 'справляются', 'такими', 'задачами', 'время', 'времени', 'лажают', 'упуская', 'какую-то', 'важную', 'информацию', 'Х', 'наоборот', 'дописывая', 'текст', 'Y', 'что-то', 'нафантазированное', 'С', 'помощью', 'модели', 'NLI', 'проверять', 'X', 'следует', 'Y', 'новом', 'тексте', 'нету', 'отсебятины', 'придуманной', 'моделью', 'Y', 'следует', 'X', 'т.е', 'вся', 'информация', 'присутствовавшая', 'исходном', 'тексте', 'новом', 'также', 'отражена', 'Во-вторых', 'помощью', 'моделей', 'NLI', 'находить', 'нетривиальные', 'парафразы', 'целом', 'определять', 'смысловую', 'близость', 'текстов', 'Для', 'русского', 'языка', 'существует', 'ряд', 'моделей', 'датасетов', 'перефразированию', 'кажется', 'сделать', 'ещё', 'В', 'статье', 'Improving', 'Paraphrase', 'Detection', 'with', 'the', 'Adversarial', 'Paraphrasing', 'Task', 'предложили', 'считать', 'парафразами', 'такую', 'пару', 'предложений', 'которой', 'каждое', 'логически', 'следует', 'другого', 'это', 'весьма', 'логично', 'Поэтому', 'модели', 'NLI', 'использовать', 'сбора', 'обучающего', 'корпуса', 'парафраз', 'не-парафраз', 'стоит', 'задача', 'детекции', 'фильтрации', 'моделей', 'генерирующих', 'парафразы']\n"
     ]
    }
   ],
   "source": [
    "print(text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "ngpNxGtqKo_d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('это', 1386), ('всё', 953), ('раскольников', 567), ('очень', 382), ('соня', 267), ('человек', 248), ('разумихин', 244), ('петрович', 208), ('время', 206), ('тебе', 204), ('дело', 200), ('тотчас', 186), ('минуту', 177), ('ивановна', 175), ('сказал', 166), ('мог', 164), ('стало', 161), ('кажется', 157), ('что-то', 156), ('сказать', 155), ('стал', 151), ('знаю', 150), ('точно', 149), ('свидригайлов', 142), ('как-то', 141), ('дуня', 138), ('порфирий', 132), ('несколько', 131), ('прямо', 129), ('раскольникова', 128), ('катерина', 127), ('действительно', 124), ('всем', 122), ('совершенно', 121), ('петр', 120), ('вчера', 118), ('особенно', 117), ('руки', 116), ('весьма', 116), ('хотя', 114), ('именно', 111), ('пульхерия', 110), ('александровна', 109), ('слишком', 108), ('глаза', 108), ('хотел', 107), ('дверь', 107), ('весь', 106), ('пошел', 105), ('лицо', 103)]\n"
     ]
    }
   ],
   "source": [
    "print(Counter(text_clean).most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jzd9pOcYK4QJ"
   },
   "source": [
    "###  Стемминг в Python \n",
    "\n",
    "Самый простой способ автоматический нормализации слов в языках с морфологией — стемминг. Стемминг — это очень грубое разбиение формы на предполагаемую основу и предполагаемую флексию. \n",
    "\n",
    "Программы-стеммеры умеют превращать:\n",
    "* \"Vyshka's students coded\" в \"Vyshka student code\"\n",
    "* 'Маша поехала за грибами' в 'Маш поехал за гриб'\n",
    "* 'Даня работает в Вышке' в \"Дан работа в Вышк\"\n",
    "\n",
    "Как можно догадаться из этих примеров, стемминг — не лучшее (и крайне непопулярное) решение для языков типа русского. Он лучше подходит для английского. \n",
    "\n",
    "В NLTK есть готовая реализация стеммера для русского языка. Давайте потестируем ее"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KD6lyVLfLJDb"
   },
   "source": [
    "### NLTK стемминг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eC3HE7FsVOca"
   },
   "source": [
    "Самый известный стеммер - стеммер Портера (или snowball стеммер). \n",
    "\n",
    "Подробнее про стеммер Портера можно почитать [вот тут](https://medium.com/@eigenein/стеммер-портера-для-русского-языка-d41c38b2d340)\n",
    "\n",
    "А совсем подробнее [вот тут](http://snowball.tartarus.org/algorithms/russian/stemmer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "OrZ65rHiLGu3"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer \n",
    "stemmer = SnowballStemmer(\"russian\") # в эту переменную мы сохраним уже готовый объект-стеммер для русского"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "deFj6YY_LRZg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "университет\n",
      "мыш\n",
      "кон\n",
      "люд\n"
     ]
    }
   ],
   "source": [
    "## предположите, что выдаст?\n",
    "print(stemmer.stem('университетами'))\n",
    "print(stemmer.stem('мышам'))\n",
    "print(stemmer.stem('конями'))\n",
    "print(stemmer.stem('людей'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "k8qc06_DLTIE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['преступлен', 'наказан', 'рома', 'шест', 'част', 'эпилог', 'част', 'перв', 'i', 'начал', 'июл', 'чрезвычайн', 'жарк', 'врем', 'вечер', 'молод', 'человек', 'вышел', 'сво', 'каморк']\n"
     ]
    }
   ],
   "source": [
    "## стеммер не умеет сам токенизировать -- он работает только с отдельными словами. \n",
    "## поэтому надо идти по словам циклом\n",
    "text_stemmed = []\n",
    "for word in text_clean:\n",
    "  text_stemmed.append(stemmer.stem(word))\n",
    "\n",
    "# аналогично text_stemmed = [stemmer.stem(word) for word in text_clean[:1000]]\n",
    "print(text_stemmed[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "xr30gE4_P23o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('эт', 1638), ('все', 1004), ('сам', 649), ('раскольник', 567), ('одн', 542), ('сво', 539), ('говор', 534), ('котор', 433), ('сказа', 428), ('стал', 396), ('так', 388), ('сон', 388), ('очен', 382), ('человек', 373), ('рук', 363), ('дел', 355), ('разумихин', 353), ('друг', 311), ('ивановн', 311), ('петрович', 290), ('минут', 278), ('ваш', 274), ('мог', 245), ('слов', 237), ('вид', 234), ('лиц', 231), ('глаз', 229), ('двер', 222), ('дума', 221), ('катерин', 221), ('врем', 210), ('комнат', 210), ('голов', 210), ('дом', 208), ('дун', 207), ('мо', 204), ('замет', 204), ('теб', 204), ('порфир', 204), ('раскольников', 202), ('особен', 195), ('всем', 193), ('хотел', 191), ('тотчас', 186), ('зна', 180), ('нем', 179), ('знает', 178), ('петр', 178), ('нача', 175), ('час', 171)]\n"
     ]
    }
   ],
   "source": [
    "print(Counter(text_stemmed).most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8HSb8QdPwZU"
   },
   "source": [
    "# Доп. задание\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hx0FkoNP_MV"
   },
   "source": [
    "давайте поизвлекаем текст из датасетов и поисследуем его, например, составив частотный список (с нормализацией, удалением пунктуации, токенизацией и удалением стоп-слов)\n",
    "\n",
    "Датасет [здесь](https://raw.githubusercontent.com/AnnSenina/Python_for_CL/main/data/elonmusk.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mqIfAEZNQKfd"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('elonmusk.csv', encoding=\"utf-8\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Getf_gdxRZLO"
   },
   "outputs": [],
   "source": [
    "df[\"tweet\"] # в этой колонке нужные нам данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FVG3ZZwwRc1K"
   },
   "outputs": [],
   "source": [
    "# как достать текст из колонок датафрейма\n",
    "tweets = df[\"tweet\"].to_list()\n",
    "print(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rsq1lJH5RjxM"
   },
   "outputs": [],
   "source": [
    "# дальше работаем с переменной tweets\n",
    "# ваш код ниже\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMvhRR8spqoG2ebUAPoi/IN",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
