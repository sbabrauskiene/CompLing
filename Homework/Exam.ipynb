{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9307cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pymystem3 import Mystem\n",
    "# m = Mystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d572a31a",
   "metadata": {},
   "source": [
    "Выбрала pymorphy, а не mystem, потому что pymorphy позволяет выбирать из нескольких вариантов разбора (если автоматический не подошел). mystem предлагает неверный вариант вариант в 'Даша мыла яблоки'. pymorphy тоже, но можно выбрать нужный."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8639f0db",
   "metadata": {},
   "source": [
    "Сначала импортируем pymorphy и создадим экземпляр класса для анализатора. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74021ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f55def9",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"Мама мыла раму\", \"Даша мыла яблоки\", \"Даша очень любит маму\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743e7536",
   "metadata": {},
   "source": [
    "Напишем функцию для лемматизации элементов списка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b56db45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    words = text.split()  # Разобьем на слова, потому что pymorphy работает с отдельными словами\n",
    "    lemmas = list()  # В этот список будем складывать леммы\n",
    "    for word in words:\n",
    "        parse = morph.parse(word)  # Парсим каждое слово\n",
    "        if len(parse) > 2:  # Отловим проблемное слово, где много вариантов разбора, в которых путается pymorphy  \n",
    "            for p in parse:\n",
    "                if 'VERB' in p.tag:  # Ищем тот вариант разбора, где глагол 'мыть'\n",
    "                    lemmas.append(p.normal_form)\n",
    "        else:\n",
    "            lemmas.append(parse[0].normal_form)  # Во всех остальных случаях берем первый вариант разбора, он верный\n",
    "    lem = ' '.join(lemmas)  # По условию задачи объединяем отдельные леммы в список\n",
    "    return(lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a1c335",
   "metadata": {},
   "source": [
    "Вызовем функцию для каждого элемента корпуса. Получим нужный результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38fe228",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corpus_lem = list()\n",
    "for i in corpus:\n",
    "    corpus_lem.append(normalize(i))\n",
    "print(corpus_lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f8e7a5",
   "metadata": {},
   "source": [
    "Делим полученный лемматизированный корпус на токены и создадим словарь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766c04d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokenized = [sentence.split() for sentence in corpus_lem]\n",
    "corpus_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826a2b1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_indices = {}\n",
    "\n",
    "for sentence in corpus_tokenized:\n",
    "    for word in sentence:\n",
    "        word_indices[word] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29778ee9",
   "metadata": {},
   "source": [
    "Я костылем его заполнила значениями по порядку, может решение не изящное, но после плясок с пайморфи времени не осталось на продумывание изящности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3852d22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = 0\n",
    "for k in word_indices.keys():\n",
    "    word_indices[k] = v\n",
    "    v += 1\n",
    "word_indices  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77180291",
   "metadata": {},
   "source": [
    "Создадим словарь-гистограмму с частотностью слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e7f7bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_dictionary = {}\n",
    "\n",
    "for sentence in corpus_tokenized:\n",
    "    words = set(sentence)  # Убираем дубликаты\n",
    "    for word in words:\n",
    "        # Заполняем частотный словарь - количество документов, в которых встретилось слово \n",
    "        count_dictionary[word] = count_dictionary.get(word, 0) + 1\n",
    "count_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b714bf",
   "metadata": {},
   "source": [
    "В формуле idf, как помнится мы выяснили, применяется десятичный логарифм, а не натуральный. Вроде бы выбор десятичного или натурального логарифма на конечный результат не влияет, но я оставлю десятичный."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3cef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aeced4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "idf = {}\n",
    "for k,v in count_dictionary.items():\n",
    "    idf[k] = log10(len(corpus)/v)  # Формула для вычисления idf, данные берем из частотного словаря \n",
    "idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4794b921",
   "metadata": {},
   "source": [
    "Инициализируем векторы для каждого документа в корпусе, они пока заполнены нулевыми значениями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4da730",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = [[0] * len(idf) for i in range(len(corpus_tokenized))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c21a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4856a7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Пронумеруем\" каждый список токенов в корпусе, чтобы потом заполнить соответствующие векторы\n",
    "for n, sentence in enumerate(corpus_tokenized):\n",
    "    words_counter = Counter(sentence)\n",
    "    sentence_length = len(sentence)\n",
    "    # Пройдем по каждому слову и вычислим для него tf-idf\n",
    "    for word in words_counter:\n",
    "        word_index = word_indices[word]\n",
    "        word_tf_idf = (idf[word]/sentence_length)\n",
    "        print(word, '-->', word_tf_idf) # Посмотрим tf-idf каждого слова\n",
    "        # Заполним соответствующие ячейки векторов\n",
    "        tf_idf[n][word_index] = word_tf_idf\n",
    "tf_idf # Посмотрим на векторы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d554973",
   "metadata": {},
   "source": [
    "Теперь вычислим косинусное сходство, чтобы понять, какие предлождения ближе друг к другу. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd42e306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e4cd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = 1 - cosine(tf_idf[0], tf_idf[1])\n",
    "result2 = 1 - cosine(tf_idf[1], tf_idf[2])\n",
    "result3 = 1 - cosine(tf_idf[0], tf_idf[2])\n",
    "\n",
    "print('Первое и второе: ', result1)\n",
    "print('Второе и третье: ', result2)\n",
    "print('Первое и третье: ', result3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b9ad79",
   "metadata": {},
   "source": [
    "Ближе всего второе и третье, а также первое и третье, потому что более близки те векторы, у которых меньше косинусное расстояние."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
