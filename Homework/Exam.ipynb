{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9307cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pymystem3 import Mystem\n",
    "# m = Mystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d572a31a",
   "metadata": {},
   "source": [
    "Выбрала pymorphy, а не mystem, потому что pymorphy позволяет выбирать из нескольких вариантов разбора (если автоматический не подошел). mystem предлагает неверный вариант вариант в 'Даша мыла яблоки'. pymorphy тоже, но можно выбрать нужный."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8639f0db",
   "metadata": {},
   "source": [
    "Сначала импортируем pymorphy и создадим экземпляр класса для анализатора. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74021ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f55def9",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"Мама мыла раму\", \"Мама мыла раму\", \"Даша мыла яблоки\", \"Даша очень любит маму\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743e7536",
   "metadata": {},
   "source": [
    "Напишем функцию для лемматизации элементов списка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b56db45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    words = text.split()  # Разобьем на слова, потому что pymorphy работает с отдельными словами\n",
    "    lemmas = list()  # В этот список будем складывать леммы\n",
    "    for word in words:\n",
    "        parse = morph.parse(word)  # Парсим каждое слово\n",
    "        if len(parse) > 2:  # Отловим проблемное слово, где много вариантов разбора, в которых путается pymorphy  \n",
    "            for p in parse:\n",
    "                if 'VERB' in p.tag:  # Ищем тот вариант разбора, где глагол 'мыть'\n",
    "                    lemmas.append(p.normal_form)\n",
    "        else:\n",
    "            lemmas.append(parse[0].normal_form)  # Во всех остальных случаях берем первый вариант разбора, он верный\n",
    "    lem = ' '.join(lemmas)  # По условию задачи объединяем отдельные леммы в список\n",
    "    return(lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a1c335",
   "metadata": {},
   "source": [
    "Вызовем функцию для каждого элемента корпуса. Получим нужный результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b38fe228",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['мама мыть рама', 'мама мыть рама', 'даша мыть яблоко', 'даша очень любить мама']\n"
     ]
    }
   ],
   "source": [
    "corpus_lem = list()\n",
    "for i in corpus:\n",
    "    corpus_lem.append(normalize(i))\n",
    "print(corpus_lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f8e7a5",
   "metadata": {},
   "source": [
    "Делим полученный лемматизированный корпус на токены и создадим словарь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "766c04d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['мама', 'мыть', 'рама'],\n",
       " ['мама', 'мыть', 'рама'],\n",
       " ['даша', 'мыть', 'яблоко'],\n",
       " ['даша', 'очень', 'любить', 'мама']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_tokenized = [sentence.split() for sentence in corpus_lem]\n",
    "corpus_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "826a2b1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_indices = {}\n",
    "\n",
    "for sentence in corpus_tokenized:\n",
    "    for word in sentence:\n",
    "        word_indices[word] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29778ee9",
   "metadata": {},
   "source": [
    "Я костылем его заполнила значениями по порядку, может решение не изящное, но после плясок с пайморфи времени не осталось на продумывание изящности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3852d22f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'мама': 0,\n",
       " 'мыть': 1,\n",
       " 'рама': 2,\n",
       " 'даша': 3,\n",
       " 'яблоко': 4,\n",
       " 'очень': 5,\n",
       " 'любить': 6}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = 0\n",
    "for k in word_indices.keys():\n",
    "    word_indices[k] = v\n",
    "    v += 1\n",
    "word_indices  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77180291",
   "metadata": {},
   "source": [
    "Создадим словарь-гистограмму с частотностью слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93e7f7bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'рама': 2,\n",
       " 'мыть': 3,\n",
       " 'мама': 3,\n",
       " 'яблоко': 1,\n",
       " 'даша': 2,\n",
       " 'очень': 1,\n",
       " 'любить': 1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_dictionary = {}\n",
    "\n",
    "for sentence in corpus_tokenized:\n",
    "    words = set(sentence)  # Убираем дубликаты\n",
    "    for word in words:\n",
    "        # Заполняем частотный словарь - количество документов, в которых встретилось слово \n",
    "        count_dictionary[word] = count_dictionary.get(word, 0) + 1\n",
    "count_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b714bf",
   "metadata": {},
   "source": [
    "В формуле idf, как помнится мы выяснили, применяется десятичный логарифм, а не натуральный. Вроде бы выбор десятичного или натурального логарифма на конечный результат не влияет, но я оставлю десятичный."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b3cef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55aeced4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'рама': 0.6931471805599453,\n",
       " 'мыть': 0.28768207245178085,\n",
       " 'мама': 0.28768207245178085,\n",
       " 'яблоко': 1.3862943611198906,\n",
       " 'даша': 0.6931471805599453,\n",
       " 'очень': 1.3862943611198906,\n",
       " 'любить': 1.3862943611198906}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf = {}\n",
    "for k,v in count_dictionary.items():\n",
    "    idf[k] = log(len(corpus)/v)  # Формула для вычисления idf, данные берем из частотного словаря \n",
    "idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4794b921",
   "metadata": {},
   "source": [
    "Инициализируем векторы для каждого документа в корпусе, они пока заполнены нулевыми значениями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e4da730",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = [[0] * len(idf) for i in range(len(corpus_tokenized))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79c21a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4856a7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "мама --> 0.09589402415059362\n",
      "мыть --> 0.09589402415059362\n",
      "рама --> 0.23104906018664842\n",
      "мама --> 0.09589402415059362\n",
      "мыть --> 0.09589402415059362\n",
      "рама --> 0.23104906018664842\n",
      "даша --> 0.23104906018664842\n",
      "мыть --> 0.09589402415059362\n",
      "яблоко --> 0.46209812037329684\n",
      "даша --> 0.17328679513998632\n",
      "очень --> 0.34657359027997264\n",
      "любить --> 0.34657359027997264\n",
      "мама --> 0.07192051811294521\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.09589402415059362, 0.09589402415059362, 0.23104906018664842, 0, 0, 0, 0],\n",
       " [0.09589402415059362, 0.09589402415059362, 0.23104906018664842, 0, 0, 0, 0],\n",
       " [0, 0.09589402415059362, 0, 0.23104906018664842, 0.46209812037329684, 0, 0],\n",
       " [0.07192051811294521,\n",
       "  0,\n",
       "  0,\n",
       "  0.17328679513998632,\n",
       "  0,\n",
       "  0.34657359027997264,\n",
       "  0.34657359027997264]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"Пронумеруем\" каждый список токенов в корпусе, чтобы потом заполнить соответствующие векторы\n",
    "for n, sentence in enumerate(corpus_tokenized):\n",
    "    words_counter = Counter(sentence)\n",
    "    sentence_length = len(sentence)\n",
    "    # Пройдем по каждому слову и вычислим для него tf-idf\n",
    "    for word in words_counter:\n",
    "        word_index = word_indices[word]\n",
    "        word_tf_idf = (idf[word]/sentence_length)\n",
    "        print(word, '-->', word_tf_idf) # Посмотрим tf-idf каждого слова\n",
    "        # Заполним соответствующие ячейки векторов\n",
    "        tf_idf[n][word_index] = word_tf_idf\n",
    "tf_idf # Посмотрим на векторы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d554973",
   "metadata": {},
   "source": [
    "Теперь вычислим косинусное сходство, чтобы понять, какие предлождения ближе друг к другу. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd42e306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4e4cd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первое и второе:  1\n",
      "Второе и третье:  0.06532091836893661\n",
      "Первое и третье:  0.06532091836893661\n",
      "Первое и четвертое:  0.04905171616697024\n"
     ]
    }
   ],
   "source": [
    "result1 = 1 - cosine(tf_idf[0], tf_idf[1])\n",
    "result2 = 1 - cosine(tf_idf[1], tf_idf[2])\n",
    "result3 = 1 - cosine(tf_idf[0], tf_idf[2])\n",
    "result4 = 1 - cosine(tf_idf[0], tf_idf[3])\n",
    "\n",
    "print('Первое и второе: ', result1)\n",
    "print('Второе и третье: ', result2)\n",
    "print('Первое и третье: ', result3)\n",
    "print('Первое и четвертое: ', result4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b9ad79",
   "metadata": {},
   "source": [
    "Ближе всего первое и второе предложение."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
