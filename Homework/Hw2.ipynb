{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AnnSenina/Python_for_CL/blob/main/homeworks/Hw2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YrHvu0z94BQ2"
   },
   "source": [
    "#Работа с файлами, pandas + парсинг\n",
    "Собираем корпус эко-новостей с сайта новостного издания \"Ведомости\"\n",
    "\n",
    "[По этой ссылке можно найти главную страницу](https://www.vedomosti.ru/ecology)\n",
    "\n",
    "Что нам предстоит сделать:\n",
    "\n",
    "достать все заголовки новостей в главной страницы + текст каждой новости\n",
    "\n",
    "сохранить в датафрейм с колонками \"ссылка\", \"дата\", \"заголовок\", \"текст\"\n",
    "\n",
    "сохранить датафрейм в файл\n",
    "\n",
    "положить код и получившийся файл в свой репозиторий на гитхабе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Способ первый, классический, как нас научили"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kQUDEUeq39_d"
   },
   "outputs": [],
   "source": [
    "import requests # Импортируем нужные библиотеки\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.vedomosti.ru/ecology/'\n",
    "page = requests.get(url) # Запрашиваем страницу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page # Проверяем, что страница вернула код 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.text, 'html') # Парсим полученную страницу супом\n",
    "#print(soup.prettify()) # Смотрим на ее структуру в красивом виде"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.vedomosti.ru/ecology/release/2022/12/08',\n",
       " 'https://www.vedomosti.ru/ecology/release/2022/11/10',\n",
       " 'https://www.vedomosti.ru/ecology/release/2022/09/08',\n",
       " 'https://www.vedomosti.ru/ecology/release/2022/07/21']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Найдем ссылки на выпуски новостей и создадим из них список\n",
    "# Пути относительные, а нам надо абсолютные. Добавим к ссылкам нужную часть ссылки  \n",
    "abs_urls = []\n",
    "for link in soup.find_all('a'):\n",
    "    if 'release' in link.get('href'):\n",
    "        abs_url = 'https://www.vedomosti.ru' + link.get('href')\n",
    "        if abs_url not in abs_urls: # Добавим условие, чтобы исключить повторное добавление ссылок\n",
    "            abs_urls.append(abs_url)\n",
    "abs_urls # Проверим, как заполнился список"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вариант 1, используем функцию для обработки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Эта функция достает из страницы нужную информацию:\n",
    "# дату, заголовки, ссылки, если это страница выпуска, или же только текст, если это страница новости)\n",
    "\n",
    "def EcoSpyder(url):\n",
    "    \n",
    "    issue_page = requests.get(url) # Запросили страницу\n",
    "    issue_soup = BeautifulSoup(issue_page.text) # Обработаем каждую страницу выпуска новостей по ссылке супом\n",
    "\n",
    "    # Проверим, что за страница, если выпуск, то вытащим дату, ссылки, заголовки\n",
    "\n",
    "    if 'release' in url:\n",
    "        dates, titles, links = [], [], []\n",
    "        \n",
    "        for d in issue_soup.find_all('time'):\n",
    "            dates.append(d.text.strip())\n",
    "        #dates = issue_soup.find('time').text  # Дата лежит в теге <time>. В выпуске все новости с одной датой, поэтому достаточно метода find.\n",
    "        \n",
    "        #date_raw = issue_soup.find('title').text # В качестве альтернативы дату можно достать регулярками из <title>\n",
    "        #date = re.findall(r'[0-9]{2}\\.[0-9]{2}\\.[0-9]{4}', date_raw) \n",
    "    \n",
    "        for k in issue_soup.find_all('a'): # Искомые заголовки и ссылки на сами тексты новостей находятся в тегах  <a>\n",
    "            if k.get('data-vr-title') != None: # Название новости лежит в теге <data-vr-title>, поэтому из всех тегов <a> найдем те, где этот тег заполнен\n",
    "                titles.append(k.get('data-vr-title')) \n",
    "                links.append('https://www.vedomosti.ru' + k.get('href')) # Сделаем ссылки абсолютными\n",
    "                v = titles + links\n",
    "                \n",
    "        return dates, v #titles, links\n",
    "   \n",
    "    else: # На странице новости нам нужен только текст новости\n",
    "        body = []\n",
    "        \n",
    "        for k in issue_soup.find_all('p', {'class': 'box-paragraph__text'}): # Текст лежит в тегах <p> с этим атрибутом \n",
    "            body.append(k.text)\n",
    "            final_body = ' '.join(body) # Объединим строки в сплошной текст\n",
    "        \n",
    "        return final_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['08.12.2022', '08.12.2022', '08.12.2022', '08.12.2022'], ['Энергетика с нулевым следом', '«Россия» станет самым мощным ледоколом в мире', 'Альтернатива «большому» атому', 'Атомная экология', 'https://www.vedomosti.ru/ecology/climate/articles/2022/12/08/954315-energetika-nulevim-sledom', 'https://www.vedomosti.ru/ecology/science_and_technology/articles/2022/12/08/954309-rossiya-stanet-samim-moschnim-ledokolom-v-mire', 'https://www.vedomosti.ru/ecology/science_and_technology/articles/2022/12/08/954298-alternativa-bolshomu-atomu', 'https://www.vedomosti.ru/ecology/protection_nature/articles/2022/12/08/954286-atomnaya-ekologiya'])\n",
      "(['10.11.2022', '10.11.2022', '10.11.2022', '10.11.2022'], ['Россия теплеет вдвое быстрее, чем остальной мир', 'Запретят ли собирать кедровые орехи', '«В России отсутствует специализированная инфраструктура сбора текстильных отходов»', '«Рубка байкальского леса приведет к непоправимым последствиям для экосистемы озера»', 'https://www.vedomosti.ru/ecology/climate/articles/2022/11/10/949670-rossiya-tepleet-vdvoe-bistree-chem-ostalnoi-mir', 'https://www.vedomosti.ru/ecology/regulation/articles/2022/11/10/949661-zapretyat-li-sobirat-kedrovie-orehi', 'https://www.vedomosti.ru/ecology/protection_nature/articles/2022/11/10/949663-v-rossii-otsutstvuet-spetsializirovannaya-infrastruktura-sbora-tekstilnih-othodov', 'https://www.vedomosti.ru/ecology/protection_nature/columns/2022/11/10/949648-rubka-baikalskogo-lesa-privedet-k-nepopravimim-posledstviyam-dlya-ekosistemi-ozera'])\n",
      "(['08.09.2022', '08.09.2022', '08.09.2022', '08.09.2022'], ['Весной и летом 2022 г. леса в России горели меньше, чем в среднем в XXI веке', '«Мониторинг Байкала — это не академическая наука, а участь сантехника»', 'Медотходы класса А могут приравнять к офисному и бытовому мусору', 'Благотворительные продукты освободят от НДС', 'https://www.vedomosti.ru/ecology/protection_nature/articles/2022/09/08/939891-vesnoi-i-letom-2022-g-lesa-v-rossii-goreli-menshe-chem-v-srednem-v-xxi-veke', 'https://www.vedomosti.ru/ecology/protection_nature/articles/2022/09/08/939876-monitoring-baikala-eto-ne-akademicheskaya-nauka-a-uchast-santehnika', 'https://www.vedomosti.ru/ecology/regulation/articles/2022/09/08/939853-medothodi-klassa-a-mogut-priravnyat-k-ofisnomu-i-bitovomu-musoru', 'https://www.vedomosti.ru/ecology/regulation/articles/2022/09/07/939765-blagotvoritelnie-produkti-osvobodyat-ot-nds'])\n",
      "(['21.07.2022', '21.07.2022', '21.07.2022', '21.07.2022'], ['Новые тренды в использовании бумаги и картона в России', '«Мы занимаемся экологическим просвещением везде, где представлена компания»', 'Экогород с видом на Байкал', 'В России будет развернута национальная система мониторинга многолетней мерзлоты', 'https://www.vedomosti.ru/ecology/science_and_technology/articles/2022/07/21/932380-novie-trendi-v-ispolzovanii-bumagi-i-kartona-v-rossii', 'https://www.vedomosti.ru/ecology/science_and_technology/articles/2022/07/21/932356-mi-zanimaemsya-ekologicheskim-prosvescheniem-vezde-gde-predstavlena-kompaniya', 'https://www.vedomosti.ru/ecology/protection_nature/articles/2022/07/21/932364-ekogorod-s-vidom-na-baikal', 'https://www.vedomosti.ru/ecology/climate/columns/2022/07/21/932367-v-rossii-budet-razvernuta-natsionalnaya-sistema-monitoringa-mnogoletnei-merzloti'])\n",
      "[['08.12.2022', '08.12.2022', '08.12.2022', '08.12.2022'], ['10.11.2022', '10.11.2022', '10.11.2022', '10.11.2022'], ['08.09.2022', '08.09.2022', '08.09.2022', '08.09.2022'], ['21.07.2022', '21.07.2022', '21.07.2022', '21.07.2022']]\n",
      "(['Энергетика с нулевым следом', '«Россия» станет самым мощным ледоколом в мире', 'Альтернатива «большому» атому', 'Атомная экология', 'https://www.vedomosti.ru/ecology/climate/articles/2022/12/08/954315-energetika-nulevim-sledom', 'https://www.vedomosti.ru/ecology/science_and_technology/articles/2022/12/08/954309-rossiya-stanet-samim-moschnim-ledokolom-v-mire', 'https://www.vedomosti.ru/ecology/science_and_technology/articles/2022/12/08/954298-alternativa-bolshomu-atomu', 'https://www.vedomosti.ru/ecology/protection_nature/articles/2022/12/08/954286-atomnaya-ekologiya'],) (['Россия теплеет вдвое быстрее, чем остальной мир', 'Запретят ли собирать кедровые орехи', '«В России отсутствует специализированная инфраструктура сбора текстильных отходов»', '«Рубка байкальского леса приведет к непоправимым последствиям для экосистемы озера»', 'https://www.vedomosti.ru/ecology/climate/articles/2022/11/10/949670-rossiya-tepleet-vdvoe-bistree-chem-ostalnoi-mir', 'https://www.vedomosti.ru/ecology/regulation/articles/2022/11/10/949661-zapretyat-li-sobirat-kedrovie-orehi', 'https://www.vedomosti.ru/ecology/protection_nature/articles/2022/11/10/949663-v-rossii-otsutstvuet-spetsializirovannaya-infrastruktura-sbora-tekstilnih-othodov', 'https://www.vedomosti.ru/ecology/protection_nature/columns/2022/11/10/949648-rubka-baikalskogo-lesa-privedet-k-nepopravimim-posledstviyam-dlya-ekosistemi-ozera'],) (['Весной и летом 2022 г. леса в России горели меньше, чем в среднем в XXI веке', '«Мониторинг Байкала — это не академическая наука, а участь сантехника»', 'Медотходы класса А могут приравнять к офисному и бытовому мусору', 'Благотворительные продукты освободят от НДС', 'https://www.vedomosti.ru/ecology/protection_nature/articles/2022/09/08/939891-vesnoi-i-letom-2022-g-lesa-v-rossii-goreli-menshe-chem-v-srednem-v-xxi-veke', 'https://www.vedomosti.ru/ecology/protection_nature/articles/2022/09/08/939876-monitoring-baikala-eto-ne-akademicheskaya-nauka-a-uchast-santehnika', 'https://www.vedomosti.ru/ecology/regulation/articles/2022/09/08/939853-medothodi-klassa-a-mogut-priravnyat-k-ofisnomu-i-bitovomu-musoru', 'https://www.vedomosti.ru/ecology/regulation/articles/2022/09/07/939765-blagotvoritelnie-produkti-osvobodyat-ot-nds'],) (['Новые тренды в использовании бумаги и картона в России', '«Мы занимаемся экологическим просвещением везде, где представлена компания»', 'Экогород с видом на Байкал', 'В России будет развернута национальная система мониторинга многолетней мерзлоты', 'https://www.vedomosti.ru/ecology/science_and_technology/articles/2022/07/21/932380-novie-trendi-v-ispolzovanii-bumagi-i-kartona-v-rossii', 'https://www.vedomosti.ru/ecology/science_and_technology/articles/2022/07/21/932356-mi-zanimaemsya-ekologicheskim-prosvescheniem-vezde-gde-predstavlena-kompaniya', 'https://www.vedomosti.ru/ecology/protection_nature/articles/2022/07/21/932364-ekogorod-s-vidom-na-baikal', 'https://www.vedomosti.ru/ecology/climate/columns/2022/07/21/932367-v-rossii-budet-razvernuta-natsionalnaya-sistema-monitoringa-mnogoletnei-merzloti'],)\n"
     ]
    }
   ],
   "source": [
    "#dates, titles, links, news_texts = [], [], [], [] # Создадим списки ключей и значений для словаря, из которого потом сделаем фрейм\n",
    "keys, values = [], []\n",
    "\n",
    "for i in range(len(abs_urls)): # Перебираем каждую ссылку на выпуск новостей\n",
    "    newspiece = EcoSpyder(abs_urls[i]) # Вытаскиваем из нее нужную инфо\n",
    "    \n",
    "    #dates.append(newspiece[0])\n",
    "    #titles.append(newspiece[1])\n",
    "    #links.append(newspiece[2])\n",
    "    print(newspiece)\n",
    "    keys.append(newspiece[0]) # Распихиваем результат по спискам: дата - в ключи, она будет индексом\n",
    "    values.append(newspiece[1:]) # Распихиваем результат по спискам: все остальное - в значения\n",
    "\n",
    "print(keys)\n",
    "print(*values)\n",
    "\n",
    "#for l in range(len(values)): # Пройдем по словарю, по каждой дате выпуска \n",
    "#for m in range(len(values[1])): # Пройдем по каждой ссылке в каждом выпуске \n",
    "#    print(values[m])\n",
    "#    news = EcoSpyder(values[m]) # Вытащим тексты\n",
    "#    news_texts.append(news) # Добавим их в список\n",
    "\n",
    "#news_dict = {keys[x]: list(values[x]) for x in range(len(keys))} # Скомпонуем словарь\n",
    "#news_dict\n",
    "        \n",
    "#dates = [b for a in dates for b in a]\n",
    "#titles = [b for a in titles for b in a]\n",
    "#links = [b for a in links for b in a]\n",
    "#print(*links) # Посмотрим, что на выходе.\n",
    "#print(len(titles))\n",
    "#print(len(news_texts))\n",
    "#print(news_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "news_dict = dict((z[0], list(z[1:])) for z in zip(dates, values))\n",
    "#values = list(zip(links, titles, news_texts))\n",
    "#for i in range(len(values)):\n",
    "#    list(values[i])\n",
    "#print(values)\n",
    "print(news_dict)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dict = {zip(dates, values)}\n",
    "news_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*zip(list2, list3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(zip(list1, zip(list2, list3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(zip(list1, zip(map(int, list2), map(int, list3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(zip(list1, map(list, zip(map(int, list2), map(int, list3)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(zip(list1, map(list, zip(*(map(int, lst) for lst in (list2, list3, list4))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "news_df = pd.DataFrame.from_dict(news_dict, orient=\"columns\").T.rename(columns={0:\"city\", 1:\"age\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "news_df = pd.DataFrame()\n",
    "#news_df = pd.DataFrame(values, index=dates, columns=['title', 'link'])\n",
    "display(news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(news_dict).T\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN8KS3CapNABRkb8+XX0Vpv",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
